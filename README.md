# ğŸš— Car Sales Data Pipeline

Pipeline incremental de datos de ventas de automÃ³viles con Airflow, Snowflake, dbt y Streamlit.

## ğŸ“‹ DescripciÃ³n

Pipeline end-to-end que extrae datos mensuales de ventas desde CSVs, los carga en Snowflake, los transforma con dbt usando arquitectura medallion, y los visualiza en un dashboard interactivo.

## ğŸ› ï¸ Stack TecnolÃ³gico

- **OrquestaciÃ³n**: Apache Airflow 2.10.3
- **Data Warehouse**: Snowflake
- **TransformaciÃ³n**: dbt 1.10.15
- **VisualizaciÃ³n**: Streamlit + Plotly + Polars
- **ContainerizaciÃ³n**: Docker & Docker Compose
- **CI/CD**: GitHub Actions

## ğŸ—ï¸ Arquitectura

```
data_lake (CSV files)
    â†“
Apache Airflow (Orchestration)
    â†“
Snowflake Data Warehouse
    â”œâ”€â”€ RAW (Landing Zone)
    â”œâ”€â”€ STAGING (Cleaned Data)
    â”œâ”€â”€ INTERMEDIATE (Business Logic)
    â””â”€â”€ MARTS (Analytics-Ready)
    â†“
Streamlit Dashboard
```

## ğŸš€ Quick Start

### Prerrequisitos

- Docker y Docker Compose
- Cuenta de Snowflake con credenciales

### ConfiguraciÃ³n

1. **Clonar el repositorio**

```bash
git clone https://github.com/NavierCracco/car-sales-data-pipeline.git
cd car-sales-data-pipeline
```

2. **Descargar el dataset**

Descarga los archivos CSV desde [Kaggle](https://www.kaggle.com/datasets/flaviocesarsandoval/car-sales-etl) y colÃ³calos en `data_lake/car_sales_data/`.

3. **Configurar Snowflake**

Ejecuta el siguiente script en un Worksheet de Snowflake para preparar el entorno:

```sql
USE ROLE SYSADMIN;

-- Crear almacÃ©n y base de datos
CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH WITH WAREHOUSE_SIZE = 'X-SMALL';
CREATE DATABASE IF NOT EXISTS CAR_SALES_DB;

-- Crear el esquema donde aterrizan los datos
CREATE SCHEMA IF NOT EXISTS CAR_SALES_DB.RAW;
```

4. **Configurar variables de entorno**

Crear archivo `.env`:

```env
# Postgres
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# Snowflake
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ROLE=your_role
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=CAR_SALES_DB
SNOWFLAKE_SCHEMA=PUBLIC
```

5. **Levantar servicios**

```bash
docker compose build
docker compose up -d
```

6. **Acceder a las interfaces**

- **Airflow**: http://localhost:8080 (admin/admin)
- **Dashboard**: http://localhost:8501

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ extract_load/src/
â”‚   â”‚   â””â”€â”€ ingest_to_snowflake.py    # Script de ingesta
â”‚   â””â”€â”€ transform/dbt_project/         # Modelos dbt
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ A_Sources/             # DefiniciÃ³n de fuentes
â”‚           â”œâ”€â”€ B_Staging/             # Limpieza y tipado
â”‚           â”œâ”€â”€ C_Intermediate/        # LÃ³gica de negocio
â”‚           â””â”€â”€ D_Marts/               # Tablas analÃ­ticas
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ car_sales_pipeline.py         # DAG de Airflow
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                         # AplicaciÃ³n Streamlit
â”‚   â”œâ”€â”€ data.py                        # ConexiÃ³n a Snowflake
â”‚   â”œâ”€â”€ utils.py                       # Funciones auxiliares
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data_lake/car_sales_data/          # Archivos CSV fuente
â”œâ”€â”€ infra/airflow/
â”‚   â”œâ”€â”€ Dockerfile                     # Imagen custom de Airflow
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                          # Tests unitarios
â”‚   â””â”€â”€ integration_test_snowflake.py  # Test de integraciÃ³n
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

## ğŸ“Š Pipeline de Datos

### DAG: `car_sales_data_pipeline`

```
start â†’ ingest_to_snowflake â†’ dbt_build â†’ end
```

**ConfiguraciÃ³n:**

- **Schedule**: `@monthly` (primer dÃ­a de cada mes)
- **Start Date**: 2018-03-01
- **Catchup**: `True` (procesa datos histÃ³ricos)
- **Retries**: 1

### 1. Ingesta (Extract & Load)

Lee archivos CSV mensuales (`car_sales_data_YYYY_M.csv`) y los carga en `CAR_SALES_DB.RAW.CAR_SALES`.

### 2. TransformaciÃ³n (dbt)

**Capas:**

- **Staging**: Limpieza y estandarizaciÃ³n (`stg_car_sales`)
- **Intermediate**: CÃ¡lculos de negocio (`int_sales_prep`)
- **Marts**: Tablas analÃ­ticas
  - `dim_cars`: DimensiÃ³n de automÃ³viles
  - `dim_salespeople`: DimensiÃ³n de vendedores
  - `fct_car_sales`: Tabla de hechos
  - `obt_car_sales`: One Big Table para dashboards

### 3. VisualizaciÃ³n

Dashboard con:

- **KPIs**: Ingresos totales, comisiones pagadas, ticket promedio, unidades vendidas
- **GrÃ¡ficos**: Tendencia mensual de ventas, ranking de vendedores
- **Filtros**: Por marca y rango de fechas
- **Detalle**: Tabla transaccional completa

## ğŸ§ª Testing & CI/CD

### Tests Locales

```bash
# Tests unitarios
python3 -m unittest discover -s tests/unit

# Test de integraciÃ³n Snowflake
python3 tests/integration_test_snowflake.py
```

### GitHub Actions

El pipeline CI/CD ejecuta automÃ¡ticamente:

1. Linting con flake8
2. Tests unitarios
3. Test de integraciÃ³n con Snowflake
4. ValidaciÃ³n de modelos dbt

## ğŸ”§ Comandos Ãštiles

```bash
# Ver logs de Airflow
docker compose logs airflow-scheduler --tail=50

# Ejecutar DAG manualmente
docker compose exec airflow-scheduler airflow dags trigger car_sales_data_pipeline

# Ejecutar dbt manualmente
docker compose exec airflow-scheduler bash
cd /opt/airflow/code/transform/dbt_project
/opt/dbt_venv/bin/dbt build --profiles-dir .

# Reiniciar servicios
docker compose restart

# Detener todo
docker compose down
```

## ğŸ› ï¸ Troubleshooting

### Airflow no detecta el DAG

```bash
docker compose restart airflow-scheduler
```

### Error de conexiÃ³n a Snowflake

```bash
# Verificar variables de entorno
docker compose exec airflow-webserver env | grep SNOWFLAKE
```

### Dashboard no carga datos

```bash
docker compose logs streamlit-app
```

## ğŸ‘¤ Autor

**Navier Cracco**
